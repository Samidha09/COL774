{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes (Using Multinomial Event Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD AND PRE-PROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(train_file_name, test_file_name):    \n",
    "    train_data = pd.read_json(train_file_name, lines=True)\n",
    "    test_data = pd.read_json(test_file_name, lines=True)\n",
    "\n",
    "    #final data\n",
    "    #TRAIN DATA\n",
    "    x_train = train_data['reviewText']\n",
    "    y_train = train_data['overall']\n",
    "    #TEST DATA\n",
    "    x_test = test_data['reviewText']\n",
    "    y_test = test_data['overall']\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLEAN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctution(tokens):\n",
    "    words = []\n",
    "    for word in tokens:\n",
    "            if word.isalpha():\n",
    "                words.append(word.lower()) #to not treat uppercase words differently\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(tokens):\n",
    "    words = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    #print(stop_words)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #porter = PorterStemmer()\n",
    "    for word in tokens:\n",
    "            if word not in stop_words: #removing stop words\n",
    "                words.append(lemmatizer.lemmatize(word))\n",
    "                #words.append(porter.stem(word))    \n",
    "                                        \n",
    "    # Stemming refers to the process of reducing each word to its root or base.\n",
    "    # I will be doing lemmatization rather than stemming, because lemmatization of words is based on linguistics and words are more meaningful.\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPARE REQUIRED DICTIONARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_prepare(x_train, y_train, saved = True, clean=False, part='1_a'):\n",
    "    if(saved):\n",
    "        #load dict from pickle file\n",
    "        f1 = open('class_words_dict_'+part+'.pickle', 'rb')\n",
    "        class_words_dict = pickle.load(f1)\n",
    "        f1.close()\n",
    "\n",
    "        #load vocab from pickle file\n",
    "        f2 = open('vocabulary_'+part+'.pickle', 'rb')\n",
    "        vocab = pickle.load(f2)\n",
    "        f2.close()\n",
    "        \n",
    "        #load vocab from pickle file\n",
    "        f3 = open('class_num_words_dict_'+part+'.pickle', 'rb')\n",
    "        class_num_words_dict = pickle.load(f3)\n",
    "        f3.close()\n",
    "        \n",
    "        return vocab, class_words_dict, class_num_words_dict\n",
    "    \n",
    "    #set of all disctinct words in the training data\n",
    "    vocab = set()\n",
    "    #number of examples in training data\n",
    "    m = len(y_train)\n",
    "    #making dictionary of words per class: key=class, val=dict(key=word, val=frequency)\n",
    "    class_words_dict = {}\n",
    "    #total words in class key: key=class, val=sum of total number of words in all examples of class key\n",
    "    class_num_words_dict = {}\n",
    "    \n",
    "    for i in range(m):\n",
    "        doc = x_train[i]\n",
    "        cls = y_train[i] #class\n",
    "        \n",
    "        #split doc into list of individual words such that punctuations are kept separate from word\n",
    "        tokens = word_tokenize(doc)\n",
    "        #removing punctuations to get final list of words\n",
    "        tokens = remove_punctution(tokens)\n",
    "        #further do stemming, removing stopwords etc. for part (d)\n",
    "        if(clean):\n",
    "            tokens = clean_data(tokens)\n",
    "        #calculating total number of words of class cls\n",
    "        if(cls in class_num_words_dict.keys()):\n",
    "            class_num_words_dict[cls] += len(tokens)\n",
    "        else:\n",
    "            class_num_words_dict[cls] = len(tokens)\n",
    "            \n",
    "        for word in tokens:\n",
    "            vocab.add(word)\n",
    "            if(cls in class_words_dict.keys()):\n",
    "                if(word in class_words_dict[cls].keys()):\n",
    "                    class_words_dict[cls][word] += 1 #if word is present, increase frequency by 1\n",
    "                else: #make frequency of word 1 since word encountered for the first time for class 'cls'\n",
    "                    class_words_dict[cls][word] = 1             \n",
    "            else: #class is encountered for the first time\n",
    "                class_words_dict[cls] = {} #initialize dictionary at class 'cls' as key\n",
    "                #since dictionary is newly initialized, word can't possibly exist in it, therefore no need to check\n",
    "                #set frequency to 1\n",
    "                class_words_dict[cls][word] = 1\n",
    "    #save dict to pickle file\n",
    "    fp = open('class_words_dict_'+part+'.pickle', 'wb')\n",
    "    pickle.dump(class_words_dict, fp)\n",
    "    fp.close()\n",
    "    \n",
    "    #save vocab to pickle file\n",
    "    f = open('vocabulary_'+part+'.pickle', 'wb')\n",
    "    pickle.dump(vocab, f)\n",
    "    f.close()\n",
    "    \n",
    "    #save class_num_words_dict to pickle file\n",
    "    nf = open('class_num_words_dict_'+part+'.pickle', 'wb')\n",
    "    pickle.dump(class_num_words_dict, nf)\n",
    "    nf.close()\n",
    "    \n",
    "    return vocab, class_words_dict, class_num_words_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, vocab, class_words_dict, class_num_words_dict, alpha=1.0, saved=True, part='1_a'):\n",
    "    if(saved):#if parameters have already been saved, load them\n",
    "        #load dict from pickle file\n",
    "        f4 = open('phi_'+ part +'.pickle', 'rb')\n",
    "        phi = pickle.load(f4)\n",
    "        f4.close()\n",
    "\n",
    "        #load vocab from pickle file\n",
    "        f5 = open('word_probs_per_class_' + part + '.pickle', 'rb')\n",
    "        word_probs_per_class = pickle.load(f5)\n",
    "        f5.close()\n",
    "        \n",
    "        return phi, word_probs_per_class\n",
    "        \n",
    "    # number of examples in training data\n",
    "    m = len(y_train)\n",
    "    \n",
    "    #parameters\n",
    "    phi = {} #key:class, val:prob of class = num of examples with class key/total number of examples\n",
    "    word_probs_per_class = {} #key:class, val:dict(key=word, val=prob of word occuring in class key) \n",
    "    \n",
    "    #calculating phi params\n",
    "    for i in range(m):\n",
    "        if(y_train[i] in phi.keys()):\n",
    "            phi[y_train[i]] += 1 #increase frequency of class\n",
    "        else:\n",
    "            phi[y_train[i]] = 1\n",
    "    # change frequency to probability\n",
    "    for key in phi.keys():\n",
    "        phi[key] /= m\n",
    "        phi[key] = np.log(phi[key]) #taking log to prevent underflow\n",
    "        \n",
    "    # calculating word_probs_per_class\n",
    "    mod_v = len(vocab)\n",
    "    \n",
    "    for cls in class_num_words_dict.keys():\n",
    "        total_words = class_num_words_dict[cls]\n",
    "        words_freqs = class_words_dict[cls]\n",
    "        word_probs_per_class[cls] = {} \n",
    "        for word in vocab:  # also use laplace smoothing with alpha hyperparameter\n",
    "            numerator = alpha  \n",
    "            if(word in words_freqs.keys()): #word occured in class cls\n",
    "                numerator += words_freqs[word]\n",
    "            denominator = (mod_v*alpha) + total_words\n",
    "            prob_word = numerator/denominator\n",
    "            #update probability in word_probs_per_class\n",
    "            word_probs_per_class[cls][word] = np.log(prob_word)#taking log to prevent underflow\n",
    "    \n",
    "    #save parameters\n",
    "    f6 = open('phi_'+ part + '.pickle', 'wb')\n",
    "    pickle.dump(phi, f6)\n",
    "    f6.close()\n",
    "    \n",
    "    #save vocab to pickle file\n",
    "    f7 = open('word_probs_per_class_' + part + '.pickle', 'wb')\n",
    "    pickle.dump(word_probs_per_class, f7)\n",
    "    f7.close()\n",
    "    \n",
    "    return phi, word_probs_per_class\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_test, y_test, phi, word_probs_per_class, mod_v, clean=False):\n",
    "    m = len(y_test)\n",
    "    preds = []\n",
    "    num_classes = len(phi.keys())\n",
    "    classes = list(phi.keys())\n",
    "    classes.sort()\n",
    "    \n",
    "    for i in range(m):\n",
    "        doc = x_test[i]\n",
    "        #split doc into list of individual words such that punctuations are kept separate from word\n",
    "        tokens = word_tokenize(doc)\n",
    "        #removing punctuations to get final list of words\n",
    "        tokens = remove_punctution(tokens)\n",
    "        #removing stop words and doing lemmatization of tokens\n",
    "        if(clean):\n",
    "            tokens = clean_data(tokens)\n",
    "            \n",
    "        #class_log_probs = sum_log_feature_prob + log_class_prior\n",
    "        class_log_probs = np.zeros(num_classes)\n",
    "        for cls in phi.keys():    \n",
    "            log_class_prior = phi[cls] #values already stored in log form\n",
    "            sum_log_feature_prob = 0 #np.sum(word_probs_per_class[cls])\n",
    "            for word in tokens:\n",
    "                if(word in word_probs_per_class[cls].keys()):\n",
    "                    sum_log_feature_prob += word_probs_per_class[cls][word]\n",
    "                \n",
    "            class_log_probs[cls-1] = sum_log_feature_prob + log_class_prior\n",
    "#         print(class_log_probs)  \n",
    "        final_pred = np.argmax(class_log_probs)+1 #due to zero based indexing adding +1\n",
    "        preds.append(final_pred)\n",
    "         \n",
    "    return preds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BASELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_baseline(x_test, y_test, phi):\n",
    "    preds = np.random.randint(1, 6, len(y_test)) #upper limit exclusive\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_baseline(x_test, y_test, phi):\n",
    "    probs = []\n",
    "    for key in phi.keys():\n",
    "        probs.append(phi[key])\n",
    "    majority_pred = np.argmax(np.array(probs))+1\n",
    "    preds = np.full(len(y_test), majority_pred)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, y):\n",
    "    m = len(y)\n",
    "    acc = 0\n",
    "    for i in range(m):\n",
    "        if(pred[i] == y[i]):\n",
    "            acc+=1\n",
    "    acc /= m\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(preds, y_test):\n",
    "    y_test = list(y_test)\n",
    "    conf_mat = confusion_matrix(y_test, preds)\n",
    "    df = pd.DataFrame(conf_mat,\n",
    "                     index = [1, 2, 3, 4, 5], \n",
    "                     columns = [1, 2, 3, 4, 5])\n",
    "    #Plotting the confusion matrix\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(df, annot=True)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Labels')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ### read data\n",
    "    train_file_name = './reviews_Digital_Music_5.json/Music_Review_train.json'\n",
    "    test_file_name = './reviews_Digital_Music_5.json/Music_Review_test.json'\n",
    "    x_train, y_train, x_test, y_test = read_data(train_file_name, test_file_name)\n",
    "    \n",
    "    vocab, class_words_dict, class_num_words_dict = dictionary_prepare(x_train, y_train, saved=True)\n",
    "    mod_v = len(vocab)\n",
    "    print(\"Length of vocabulary on original text data (without punctuations) : \", mod_v)\n",
    "    ### Part (a) change saved=True, after calculating params on train data once, to skip recomputing again\n",
    "    phi,  word_probs_per_class = train(x_train, y_train,  vocab, class_words_dict, class_num_words_dict, saved=True)\n",
    "    \n",
    "    ### Part (b)\n",
    "    # random baseline\n",
    "    print('\\n----------------- PART (b) ---------------------------\\n')\n",
    "    preds = random_baseline(x_test, y_test, phi)\n",
    "    test_acc = accuracy(preds, y_test)\n",
    "    print(\"Accuracy on test data using Random baseline: \", test_acc)\n",
    "    \n",
    "    #majority baseline\n",
    "    preds = majority_baseline(x_test, y_test, phi)\n",
    "    test_acc = accuracy(preds, y_test)\n",
    "    print(\"Accuracy on test data using Majority baseline: \", test_acc)\n",
    "    \n",
    "    #Multinomial Naive Bayes\n",
    "    preds = predict(x_test, y_test, phi, word_probs_per_class, mod_v)\n",
    "    test_acc = accuracy(preds, y_test)\n",
    "    print(\"Accuracy on test data using Multinomial Naive Bayes: \", test_acc)\n",
    "    \n",
    "    ### Part (c) - draw confusion matrix of test data\n",
    "    print('\\n----------------- PART (c) ---------------------------\\n')\n",
    "    #plot_confusion_matrix(preds, y_test)\n",
    "    ### Part (d) - text cleaning\n",
    "    print('\\n----------------- PART (d) ---------------------------\\n')\n",
    "    vocab, class_words_dict, class_num_words_dict = dictionary_prepare(x_train, y_train, saved=True, clean=True, part='1_d')\n",
    "    mod_v = len(vocab)\n",
    "    print(\"Length of vocabulary after cleaning text data: \", mod_v)\n",
    "    #retrain\n",
    "    phi,  word_probs_per_class = train(x_train, y_train,  vocab, class_words_dict, class_num_words_dict, saved=True, part='1_d')\n",
    "    #test\n",
    "    preds_1d = predict(x_test, y_test, phi, word_probs_per_class, mod_v, clean=True)\n",
    "    test_acc = accuracy(preds_1d, y_test)\n",
    "    print(\"Accuracy on test data using Multinomial Naive Bayes after cleaning text: \", test_acc)  \n",
    "    ### Part (e)\n",
    "    #print('\\n----------------- PART (e) ---------------------------\\n')\n",
    "    ### Part (f)\n",
    "    #print('\\n----------------- PART (f) ---------------------------\\n')\n",
    "#     print(\"F1 score: \", f1_score(list(y_test), pred, average=None))\n",
    "#     print(\"F1 score (macro-averaged): \", f1_score(list(y_test), pred, average='macro'))\n",
    "    ### Part (g)\n",
    "    #print('\\n----------------- PART (g) ---------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary on original text data (without punctuations) :  90062\n",
      "\n",
      "----------------- PART (b) ---------------------------\n",
      "\n",
      "Accuracy on test data using Random baseline:  0.2035\n",
      "Accuracy on test data using Majority baseline:  0.07757142857142857\n",
      "Accuracy on test data using Multinomial Naive Bayes:  0.6646428571428571\n",
      "\n",
      "----------------- PART (c) ---------------------------\n",
      "\n",
      "\n",
      "----------------- PART (d) ---------------------------\n",
      "\n",
      "Length of vocabulary after cleaning text data:  83277\n",
      "Accuracy on test data using Multinomial Naive Bayes after cleaning text:  0.6607142857142857\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 5 5]\n"
     ]
    }
   ],
   "source": [
    "print(np.full(3,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFERENCES\n",
    "\n",
    "1. https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "2. https://www.analyticsvidhya.com/blog/2021/06/confusion-matrix-for-multi-class-classification/\n",
    "3. https://www.geeksforgeeks.org/python-lemmatization-with-nltk/\n",
    "4. https://www.researchgate.net/publication/337321725_The_Effect_of_Stemming_and_Removal_of_Stopwords_on_the_Accuracy_of_Sentiment_Analysis_on_Indonesian-language_Texts\n",
    "5. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
