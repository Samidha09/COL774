{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001, 3)\n",
      "\n",
      "Error values on test data\n",
      "\n",
      "Error with original parameters:  [0.98294692]\n",
      "batch size:  1  -> error :  [0.99873811]\n",
      "batch size:  100  -> error :  [0.98343432]\n",
      "batch size:  10000  -> error :  [0.98327846]\n",
      "batch size:  1000000  -> error :  [0.98330276]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.animation as animation\n",
    "from ipywidgets import interactive\n",
    "from matplotlib import cm\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "np.random.seed(43)\n",
    "\n",
    "# PART A\n",
    "def sample_data():\n",
    "    m = 1000000 #number of data points\n",
    "    theta = np.array([[2], [1], [3]]) #given\n",
    "    mu_1, sigma_1 = 3, 2\n",
    "    mu_2, sigma_2 = -1, 2\n",
    "    # examples : X values\n",
    "    x_0 = np.ones(m)\n",
    "    x_1 = np.random.normal(mu_1, sigma_1, m)\n",
    "    x_2 = np.random.normal(mu_2, sigma_2, m)\n",
    "    X = np.row_stack((x_2,x_1,x_0)) # making design matrix\n",
    "    # examples: Y values\n",
    "    sigma_3 = math.sqrt(2)\n",
    "    Y = [] #np.random.normal(mu_3, sigma_3, m)\n",
    "    for i in range(m):\n",
    "        X_i = X.T[i]\n",
    "        mu_3 = np.dot(theta.T, X_i)\n",
    "        epsilon = np.random.normal(0, sigma_3)\n",
    "        Y.append(mu_3+epsilon)\n",
    "    Y = (np.array(Y)).reshape((len(Y),1))\n",
    "    return X, Y\n",
    "\n",
    "# PART B\n",
    "\n",
    "# function to calculate J(theta)\n",
    "def cost_function(Y, theta, X):\n",
    "    #number of examples for taking average\n",
    "    m = X.shape[1]\n",
    "    Y_hat = np.dot(theta.T,X)\n",
    "    #computing mse loss\n",
    "    J_theta = (1/(2*m)) * np.sum((Y - Y_hat.T)**2, axis=0)\n",
    "    return J_theta\n",
    "\n",
    "# function to calculate gradient\n",
    "def gradient(theta, X, Y):\n",
    "    #number of examples for taking average\n",
    "    m = X.shape[1]\n",
    "    Y_hat = np.dot(X.T, theta)\n",
    "    #computing gradient of mse loss\n",
    "    grad_vector = (-1/m)* np.dot(X,(Y - Y_hat))\n",
    "    return grad_vector\n",
    "\n",
    "def shuffle_data(X,Y):\n",
    "    p = np.random.permutation(X.shape[1])\n",
    "    return X[:,p], Y[p]\n",
    "\n",
    "def SGD(X, Y, learning_rate, r, delta = 0.001, k=1000):\n",
    "    m = X.shape[1]\n",
    "    X, Y = shuffle_data(X,Y)\n",
    "    theta = np.zeros((X.shape[0], 1))\n",
    "    num_batches = m/r\n",
    "    t = 1\n",
    "    timestep = 0\n",
    "    max_epochs = 50000\n",
    "    current_avg_cost = 0\n",
    "    last_avg_cost = 0\n",
    "    #for plotting purpose\n",
    "    theta_tracker = [theta]\n",
    "    cost_tracker = []\n",
    "    while(1):\n",
    "        start_idx = (t - 1)*r \n",
    "        end_idx = (t*r)\n",
    "        batch_x = X[:, start_idx:end_idx] #[start_idx, end_idx)\n",
    "        batch_y = Y[start_idx:end_idx, :]\n",
    "        #update parameters for batch\n",
    "        current_avg_cost += cost_function(batch_y, theta, batch_x)\n",
    "        theta = theta - learning_rate*gradient(theta, batch_x, batch_y)\n",
    "        theta_tracker.append(theta)\n",
    "        t = t+1 if(t<num_batches) else 1\n",
    "        if(timestep%k == 0):\n",
    "            current_avg_cost /= k #make cost average\n",
    "            cost_tracker.append(current_avg_cost)\n",
    "            if((abs(current_avg_cost -  last_avg_cost) <= delta) or (timestep >= max_epochs)):\n",
    "                break\n",
    "            last_avg_cost = current_avg_cost\n",
    "            current_avg_cost = 0\n",
    "        timestep += 1\n",
    "    \n",
    "    return (theta,theta_tracker,timestep,cost_tracker)\n",
    "\n",
    "# PART C\n",
    "\n",
    "def load_test_data():\n",
    "    X = np.genfromtxt('./Data/q2/q2test.csv', delimiter=',')\n",
    "    #adding X_0 for intercept\n",
    "#     print(X.shape)\n",
    "    X_0 = np.ones(X.shape[0]-1) #to remove label of columns as it was string and giving nan value\n",
    "    X_1 = X[1:,0]\n",
    "    X_2 = X[1:,1]\n",
    "    #extract labels\n",
    "    test_labels = (X[1:,2]).reshape((X.shape[0]-1,1))\n",
    "    #extract features\n",
    "    test_data = np.row_stack((X_2, X_1)) # each example is a column in test_data\n",
    "    test_data = np.row_stack((test_data, X_0))\n",
    "    return test_data, test_labels\n",
    "    \n",
    "\n",
    "def test(test_data, test_labels, theta, final_theta_dict):\n",
    "    original_error = cost_function(test_labels, theta, test_data)\n",
    "    test_error = {}\n",
    "    for key in final_theta_dict:\n",
    "        params = final_theta_dict[key]\n",
    "        test_error[key] = cost_function(test_labels, params, test_data)\n",
    "    return test_error, original_error\n",
    "\n",
    "# PART D\n",
    "\n",
    "def plot(parameter_list, elevation, angle, batch_size):\n",
    "    #setting up the figure size\n",
    "    fig = plt.figure(figsize = [12, 15])\n",
    "    #setting up the axes as a 3 dimensional plot\n",
    "    ax = fig.gca(projection = '3d')\n",
    "    \n",
    "    #values for plotting line\n",
    "    theta_2_vals = (parameter_list[:,0,:].flatten()).tolist()\n",
    "    theta_1_vals = (parameter_list[:,1,:].flatten()).tolist()\n",
    "    theta_0_vals = (parameter_list[:,2,:].flatten()).tolist()\n",
    "    \n",
    "    #plot the surface\n",
    "    ax.plot3D(theta_1_vals, theta_0_vals, theta_2_vals, 'red', marker='.')\n",
    "    ax.set_title('Movement of parameters at each iteration of SGD for batch size '+str(batch_size))\n",
    "    ax.set_xlabel('theta_1') \n",
    "    ax.set_ylabel('theta_0')\n",
    "    ax.set_zlabel('theta_2') \n",
    "    plt.savefig('./Results/q2/SGD_movemement_params_'+str(batch_size)+'.png')\n",
    "    plt.clf()\n",
    "    \n",
    "\n",
    "def plot_params(parameter_dict):\n",
    "    for key in parameter_dict.keys():\n",
    "        parameter_list = np.array(parameter_dict[key])\n",
    "        plot (parameter_list, 5, 15, key)\n",
    "\n",
    "# UTILITY FUNCTIONS\n",
    "\n",
    "def plot_cost(cost, num_iter, k, batch_size):\n",
    "    x_s = []\n",
    "    i=0\n",
    "    while(i<=num_iter):\n",
    "        x_s.append(i)\n",
    "        i+=k\n",
    "    plt.title(\"Cost vs timesteps\")\n",
    "    plt.xlabel(\"timesteps\")\n",
    "    plt.ylabel('Cost')\n",
    "    plt.plot(x_s, cost)\n",
    "    plt.savefig('./Results/q2/SGD_cost_vs_timestep'+str(batch_size)+'.png')\n",
    "    plt.clf()\n",
    "\n",
    "def save_pickle_files(final_theta_dict, parameter_dict, num_iter_dict, cost_dict):\n",
    "    f1 = open('./parameter_dict.pickle', 'wb')\n",
    "    pickle.dump(parameter_dict, f1)\n",
    "    f2 = open('./final_theta_dict.pickle', 'wb')\n",
    "    pickle.dump(final_theta_dict, f2)\n",
    "    f3 = open('./num_iter_dict.pickle', 'wb')\n",
    "    pickle.dump(num_iter_dict, f3)\n",
    "    f4 = open('./cost_dict.pickle', 'wb')\n",
    "    pickle.dump(cost_dict, f4)\n",
    "\n",
    "def load_pickle_files():\n",
    "    f1 = open('./parameter_dict.pickle', 'rb')\n",
    "    parameter_dict = pickle.load(f1)\n",
    "    f2 = open('./final_theta_dict.pickle', 'rb')\n",
    "    final_theta_dict = pickle.load(f2)\n",
    "    f3 = open('./num_iter_dict.pickle', 'rb')\n",
    "    num_iter_dict = pickle.load(f3)\n",
    "    f4 = open('./cost_dict.pickle', 'rb')\n",
    "    cost_dict = pickle.load(f4)\n",
    "    return (final_theta_dict, parameter_dict, num_iter_dict, cost_dict) \n",
    "\n",
    "# Main function\n",
    "\n",
    "def main():\n",
    "    #get training dataset - part(a)\n",
    "    X, Y = sample_data()\n",
    "    \n",
    "    #part(b) SGD\n",
    "    theta = np.array([[2], [1], [3]]) #theta2, theta1, theta0\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    #various configurations for stopping criteria of SGD\n",
    "    delta = {1:10**(-3), 100:10**(-4), 10000:10**(-6), 1000000:10**(-8)}\n",
    "    k = {1:300, 100:150, 10000:100, 1000000:1}\n",
    "    batch_sizes = [1, 100, 10000, 1000000]\n",
    "    \n",
    "    final_theta_dict = {} #key:batch_size, val: list of parameters\n",
    "    parameter_dict = {} #key:batch_size, val: list of list(3*1) of parameters\n",
    "    num_iter_dict = {} #key:batch_size, val: num_iterations_for_convergence\n",
    "    cost_dict = {} #key:batch_size, val: list of average costs of k batches\n",
    "    time_taken = {} #key:batch_size, val: time taken in seconds\n",
    "    for batch_size in batch_sizes:\n",
    "        start_time = time.time()\n",
    "        final_theta_dict[batch_size],  parameter_dict[batch_size], num_iter_dict[batch_size], cost_dict[batch_size] = SGD(X, Y, learning_rate, batch_size, delta[batch_size], k[batch_size])\n",
    "        time_taken[batch_size] = time.time() - start_time\n",
    "        #plot cost vs iterations\n",
    "        print(\"done: \", batch_size)\n",
    "        plot_cost(cost_dict[batch_size], num_iter_dict[batch_size],  k[batch_size], batch_size)\n",
    "        \n",
    "    save_pickle_files(final_theta_dict, parameter_dict, num_iter_dict, cost_dict)\n",
    "#     final_theta_dict, parameter_dict, num_iter_dict, cost_dict = load_pickle_files()\n",
    "    \n",
    "#     print final_parameters\n",
    "    print('\\nFinal Parameters\\n')\n",
    "    for batch_size in batch_sizes:\n",
    "        print(\"batch size: \", batch_size, \" -> params: \", final_theta_dict[batch_size])\n",
    "#     print number of iterations taken for convergence\n",
    "    print('\\nNumber of iterations taken for convergence\\n')\n",
    "    for batch_size in batch_sizes:\n",
    "        print(\"batch size: \", batch_size, \" -> num_iter: \", num_iter_dict[batch_size])\n",
    "#     print time taken for convergence\n",
    "    print('\\nTime taken(in sec) for convergence\\n')\n",
    "    for batch_size in batch_sizes:\n",
    "        print(\"batch size: \", batch_size, \" -> time_taken: \", time_taken[batch_size])\n",
    "        \n",
    "    #print difference between learned parameters and original parameters\n",
    "    print('\\nL2 norm of original parameters and learned parameters\\n')\n",
    "    for batch_size in batch_sizes:\n",
    "        print(\"batch size: \", batch_size, \" -> norm: \", np.linalg.norm(theta-final_theta_dict[batch_size], 2))\n",
    "        \n",
    "    #part(c) Test\n",
    "    test_data, test_labels = load_test_data() \n",
    "    (test_error, original_error) = test(test_data, test_labels, theta, final_theta_dict) #dictionary, key:batch_size, value:mse error on test data\n",
    "    #print error on test data\n",
    "    print('\\nError values on test data\\n')\n",
    "    print(\"Error with original parameters: \", original_error)\n",
    "    for batch_size in batch_sizes:\n",
    "        print(\"batch size: \", batch_size, \" -> error : \", test_error[batch_size])\n",
    "        \n",
    "    #part(d) Plot\n",
    "    plot_params(parameter_dict) #Results would be stored in a directory ./Results/q4/\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
